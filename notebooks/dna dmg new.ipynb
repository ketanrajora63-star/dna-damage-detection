{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8652f109",
        "outputId": "12ff19e2-06af-47a0-d5fa-f7752503c42a"
      },
      "source": [
        "!unzip DNA_Damage_Project-20250901T154313Z-1-001.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  DNA_Damage_Project-20250901T154313Z-1-001.zip\n",
            "replace DNA_Damage_Project/image/red/WellC4_Seq0011_WellC4_Seq0011_T01_XY7_RGB_TxRED.tif? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4993f1f"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85ca3728"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826c478b"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "793ef502"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "We will define a convolutional neural network (CNN) using TensorFlow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48f6758c"
      },
      "source": [
        "# Task\n",
        "Process the red and green channel images for nucleus segmentation, normalization, foci detection, and labeling, preparing the data for model training. The red channel images are located in \"train_images/red/\" and \"test_images/red/\", and the green channel images are located in \"train_images/green/\" and \"test_images/green/\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17c0287b"
      },
      "source": [
        "## Nucleus segmentation (red channel)\n",
        "\n",
        "### Subtask:\n",
        "Implement the segmentation steps using Otsu's thresholding, watershed algorithm, and noise removal.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "327f1e07"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the image segmentation steps using Otsu's thresholding, morphological operations, watershed algorithm, and noise removal for both training and testing red channel images as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "733a548c"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    markers = cv2.watershed(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), markers)\n",
        "    image[markers == -1] = [255, 0, 0] # Mark boundary\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8fdd343"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a ValueError in the watershed step where we are trying to assign a 3-channel color to a grayscale image. The watershed function modifies the `markers` array in-place and also tries to color the original image if provided as a BGR image. The error occurs because the original `image` is grayscale, not BGR. We should create a BGR version of the grayscale image before applying watershed and then extract the mask from the modified `markers`. Also, the cleaned_mask should be a grayscale image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWa0XRd5_mXg"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01355991"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "### Subtask:\n",
        "Implement the normalization steps, including cropping, rotating, and padding the segmented nuclei.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4cfcd4"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `normalize_nucleus` function as described in the instructions to process each nucleus in a paired image and its mask, and then apply this function to the training and testing datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a8a2f0a"
      },
      "source": [
        "def normalize_nucleus(paired_image, segmented_mask, target_size=(64, 64)):\n",
        "    \"\"\"\n",
        "    Normalizes each detected nucleus in a paired image (red, green)\n",
        "    based on the provided segmented mask.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        target_size: The desired size for the normalized nucleus images.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        (red nucleus image, green nucleus image).\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    normalized_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for both channels\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "        green_roi = green_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the ROIs to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "        padded_green_roi = np.pad(green_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size\n",
        "        resized_red_roi = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "        resized_green_roi = cv2.resize(padded_green_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        normalized_nuclei.append((resized_red_roi, resized_green_roi))\n",
        "\n",
        "    return normalized_nuclei\n",
        "\n",
        "# Apply normalization to training and testing sets\n",
        "train_normalized_nuclei = []\n",
        "for paired_img, mask in zip(train_paired_images, train_segmented_masks):\n",
        "    train_normalized_nuclei.extend(normalize_nucleus(paired_img, mask))\n",
        "\n",
        "test_normalized_nuclei = []\n",
        "for paired_img, mask in zip(test_paired_images, test_segmented_masks):\n",
        "    test_normalized_nuclei.extend(normalize_nucleus(paired_img, mask))\n",
        "\n",
        "# Print the number of normalized nuclei\n",
        "print(f\"Found {len(train_normalized_nuclei)} normalized nuclei in the training set.\")\n",
        "print(f\"Found {len(test_normalized_nuclei)} normalized nuclei in the testing set.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec148330"
      },
      "source": [
        "## Green channel loading and global thresholding\n",
        "\n",
        "### Subtask:\n",
        "Load the green channel images and apply global thresholding to reduce background haze.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ee74450"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to apply global thresholding to grayscale images and then apply it to the training and testing green channel images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13424a68"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "train_green_thresholded = [apply_global_thresholding(img) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f9a2cb4"
      },
      "source": [
        "## Foci detection and nucleus labeling\n",
        "\n",
        "### Subtask:\n",
        "Implement a method to detect green foci within the segmented nuclei and label each nucleus as \"damaged\" or \"normal\" based on the presence of foci.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b447ff99"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the `detect_foci_and_label_nucleus` function and apply it to the training and testing data to detect foci and label nuclei as damaged or normal.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7e01df1"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccff9591"
      },
      "source": [
        "## Integrate processing steps\n",
        "\n",
        "### Subtask:\n",
        "Combine all the processing steps into a cohesive pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8847ca34"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function `process_image_pair` that integrates the previously defined functions for segmentation, thresholding, foci detection, and labeling, and then iterate through the paired training and testing images to process them using this function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf97a58b"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    thresholded_green_image = apply_global_thresholding(green_img)\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fde310ba"
      },
      "source": [
        "## Prepare data for model training\n",
        "\n",
        "### Subtask:\n",
        "Format the processed data (normalized red channel nuclei and their labels) for input into the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8328d0"
      },
      "source": [
        "**Reasoning**:\n",
        "Separate the normalized red nucleus images and their labels into separate lists for training and testing, then convert these lists to NumPy arrays and convert labels to numerical format, finally print the shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea316097"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b149280"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Nucleus segmentation was successfully applied to the red channel images for both training (5 images) and testing (91 images) datasets, generating corresponding segmented masks.\n",
        "*   Normalization steps, including cropping, padding, and resizing to 64x64 pixels, were applied to the detected nuclei from both red and green channels. This resulted in 30 normalized nucleus pairs in the training set and 914 in the testing set.\n",
        "*   Global thresholding was applied to the green channel images, producing thresholded versions for both training (5 images) and testing (91 images) sets.\n",
        "*   Green foci were detected within the segmented nuclei, and each nucleus was labeled as \"damaged\" or \"normal\" based on the foci count. This resulted in 6 damaged and 24 normal nuclei in the training set, and 94 damaged and 820 normal nuclei in the testing set.\n",
        "*   The entire processing pipeline, integrating segmentation, thresholding, foci detection, and labeling, was successfully implemented and applied to both datasets.\n",
        "*   The processed data was formatted for model training by separating normalized red channel nucleus images and their labels, converting them into NumPy arrays with a shape of (number\\_of\\_samples, 64, 64, 1), and mapping string labels (\"normal\", \"damaged\") to numerical values (0, 1).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The significant imbalance between \"normal\" and \"damaged\" nuclei, especially in the testing set (820 normal vs. 94 damaged), should be considered when selecting and training the classification model. Data augmentation or choosing a model robust to class imbalance may be necessary.\n",
        "*   Evaluate the sensitivity of the foci detection and nucleus labeling to the chosen `foci_threshold`. Further tuning of this parameter might improve the accuracy of the \"damaged\" versus \"normal\" classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8685279"
      },
      "source": [
        "## Feature Extraction (Shape Features)\n",
        "\n",
        "### Subtask:\n",
        "Extract shape features from the segmented and normalized red channel nucleus images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6270a311"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a function to calculate the specified shape features (Major/Minor Axis Length, Axis Ratio, Eccentricity, Solidity, Roundness/Form Factor, Hu Moments, and HOG) for each normalized nucleus image and apply it to both training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dcd2df3"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255)\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros or NaNs if no contour is found\n",
        "        return [0] * 18 # 7 Hu moments + 7 for other shape features + 4 for HOG (example size)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Basic Shape Features\n",
        "    area = cv2.contourArea(contour)\n",
        "    perimeter = cv2.arcLength(contour, True)\n",
        "\n",
        "    if area == 0:\n",
        "         # Return a list of zeros or NaNs if area is zero\n",
        "        return [0] * 18\n",
        "\n",
        "    # Fit ellipse\n",
        "    if len(contour) >= 5:\n",
        "        ellipse = cv2.fitEllipse(contour)\n",
        "        (center, axes, orientation) = ellipse\n",
        "        major_axis_length = max(axes)\n",
        "        minor_axis_length = min(axes)\n",
        "        axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "        eccentricity = np.sqrt(1 - (minor_axis_length / major_axis_length)**2) if major_axis_length > 0 else 0\n",
        "    else:\n",
        "        major_axis_length, minor_axis_length, axis_ratio, eccentricity = 0, 0, 0, 0\n",
        "\n",
        "\n",
        "    # Solidity\n",
        "    hull = cv2.convexHull(contour)\n",
        "    hull_area = cv2.contourArea(hull)\n",
        "    solidity = area / hull_area if hull_area > 0 else 0\n",
        "\n",
        "    # Roundness/Form Factor\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly, not the contour\n",
        "    # HOG requires a specific input format and parameters. This is a basic example.\n",
        "    # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "    try:\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    # Adjust the number of features to match the combined list size\n",
        "    features = [major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "688a9e6c"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "943b0e91"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0dec424"
      },
      "source": [
        "## Feature Extraction (Texture Features)\n",
        "\n",
        "### Subtask:\n",
        "Extract texture features from the segmented and normalized red channel nucleus images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f974b15"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a function to calculate texture features (e.g., Haralick texture features using `mahotas`) for each normalized nucleus image and apply it to both training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "278c88f9"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) - adjusted sizes\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04181006"
      },
      "source": [
        "## Combine Features\n",
        "\n",
        "### Subtask:\n",
        "Combine the extracted shape and texture features into a single feature vector for each nucleus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61b07c8e"
      },
      "source": [
        "**Reasoning**:\n",
        "Concatenate the shape and texture feature arrays for both the training and testing datasets to create the final feature vectors for model input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05bcd342"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    # Reshape texture features to be 2D if they are 3D (as seen in the output)\n",
        "    # The shape (30, 4, 13) suggests 4 directions with 13 features each.\n",
        "    # We can flatten the last two dimensions to get (30, 52)\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    # Reshape texture features similarly\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d93acfc"
      },
      "source": [
        "# Task\n",
        "Summarize the work done so far in the nucleus segmentation and classification project, covering data loading, preprocessing, segmentation, normalization, green channel labeling, and feature extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "715bdc56"
      },
      "source": [
        "## Train random forest classifier\n",
        "\n",
        "### Subtask:\n",
        "Train a Random Forest classifier on the extracted shape and texture features, addressing class imbalance and experimenting with hyperparameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fdf569c"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries for Random Forest and SMOTE, then apply SMOTE to the training data and train the Random Forest classifier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c34ad3b"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "# Instantiate and train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "print(\"Random Forest classifier trained successfully with SMOTE.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f66a3c04"
      },
      "source": [
        "## Evaluate random forest classifier\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained Random Forest model and analyze feature importance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25dad858"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained Random Forest model and analyze feature importance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b6cec2a"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = rf_classifier.predict(test_combined_features)\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "num_texture_features = train_texture_features.shape[1] # This is the flat size after reshaping\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Create a pandas Series for feature importances\n",
        "feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "# Print the top 20 most important features\n",
        "print(\"\\nTop 20 Most Important Features:\")\n",
        "print(sorted_feature_importance_series.head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4868e46"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a mismatch between the number of feature importances from the model (209) and the number of feature names generated (233). This is likely due to an incorrect assumption about the size of the flattened texture features. I need to correctly calculate the number of texture features after flattening to create the correct list of feature names.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lR28jqnDRex"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = rf_classifier.predict(test_combined_features)\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The original texture features shape was (samples, num_features_per_image)\n",
        "# After flattening, it should be (samples, num_features_per_image)\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "# Assuming train_texture_features_flat was successfully created:\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created due to the previous error,\n",
        "    # we need to recreate it or calculate its expected size.\n",
        "    # Based on the previous output (30, 76), the flattened size should be 76.\n",
        "    num_texture_features_flat = 76 # Manually set based on previous output\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61b359a"
      },
      "source": [
        "## Train convolutional neural network (cnn)\n",
        "\n",
        "### Subtask:\n",
        "Build and train a CNN directly on the normalized nucleus images, incorporating data augmentation, regularization, and potentially transfer learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c8dbd55"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the CNN model architecture, compile it, set up data augmentation, and train the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f7c7b5f"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model)\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model))\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a413b6a1"
      },
      "source": [
        "**Reasoning**:\n",
        "The CNN model has been built and trained. To evaluate its performance, calculate and print the classification report and confusion matrix on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWA5biWEDx3w"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59a45817"
      },
      "source": [
        "## Evaluate convolutional neural network (cnn)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained CNN.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f82561b1"
      },
      "source": [
        "**Reasoning**:\n",
        "Utilize the trained CNN model to generate predictions on the test set, convert probabilities to class labels, and then generate and print the classification report and confusion matrix to evaluate the model's performance as requested by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a6ec58a"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35b48847"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "# Instantiate and train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "print(\"Random Forest classifier trained successfully with SMOTE.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84ef1a1"
      },
      "source": [
        "## Evaluate random forest classifier\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained Random Forest model and analyze feature importance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f253c0ad"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the performance of the trained Random Forest model and analyze feature importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6445ba34"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = rf_classifier.predict(test_combined_features)\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The original texture features shape was (samples, num_features_per_image)\n",
        "# After flattening, it should be (samples, num_features_per_image)\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "# Assuming train_texture_features_flat was successfully created:\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created due to the previous error,\n",
        "    # we need to recreate it or calculate its expected size.\n",
        "    # Based on the previous output (30, 76), the flattened size should be 76.\n",
        "    num_texture_features_flat = 76 # Manually set based on previous output\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933643bd"
      },
      "source": [
        "## Train convolutional neural network (cnn)\n",
        "\n",
        "### Subtask:\n",
        "Build and train a CNN directly on the normalized nucleus images, incorporating data augmentation, regularization, and potentially transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465f9c91"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the CNN model architecture, compile it, set up data augmentation, and train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44192a19"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model)\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model))\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d150538"
      },
      "source": [
        "## Evaluate convolutional neural network (cnn)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c2ec31"
      },
      "source": [
        "**Reasoning**:\n",
        "Utilize the trained CNN model to generate predictions on the test set, convert probabilities to class labels, and then generate and print the classification report and confusion matrix to evaluate the model's performance as requested by the subtask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dba82c1"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4357e80"
      },
      "source": [
        "## Build and Train Ensemble Model\n",
        "\n",
        "### Subtask:\n",
        "Combine the CNN probabilities with the handcrafted features and train a second-level Random Forest model, including feature selection and optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5837f9e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the predictions (probabilities) from the trained CNN with the extracted shape and texture features, and then train a second-level Random Forest classifier on this combined feature set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c6a6dac"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model)\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model)\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for combined features and CNN predictions.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1348e395"
      },
      "source": [
        "## Evaluate Ensemble Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ee5e1f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Utilize the trained ensemble model to generate predictions on the test set and then generate and print the classification report and confusion matrix to evaluate its performance as requested by the subtask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3916f761"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244b12be"
      },
      "source": [
        "## Summary of Model Performance\n",
        "\n",
        "We have trained and evaluated three different models for classifying damaged nuclei: a Random Forest classifier using handcrafted features, a Convolutional Neural Network (CNN) trained on normalized images, and an ensemble model combining CNN predictions and handcrafted features.\n",
        "\n",
        "Here's a summary of the key performance metrics for each model on the test set:\n",
        "\n",
        "### Random Forest Classifier (using handcrafted features with SMOTE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cdff5a4"
      },
      "source": [
        "# Print the classification report and confusion matrix for the Random Forest model again for easy comparison\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf4391fc"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf7eae04"
      },
      "source": [
        "# Print the classification report and confusion matrix for the CNN model again for easy comparison\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"CNN Evaluation:\")\n",
        "# Ensure model and test_images_for_model are available from previous steps\n",
        "if 'model' in locals() and 'test_images_for_model' in locals():\n",
        "    predictions_prob_cnn = model.predict(test_images_for_model)\n",
        "    predictions_cnn = (predictions_prob_cnn > 0.5).astype(\"int32\")\n",
        "    print(classification_report(test_labels_for_model, predictions_cnn))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels_for_model, predictions_cnn))\n",
        "else:\n",
        "    print(\"CNN model or test data not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a058540"
      },
      "source": [
        "### Ensemble Model (CNN probabilities + Handcrafted Features with SMOTE and Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d25e05eb"
      },
      "source": [
        "# Print the classification report and confusion matrix for the Ensemble model again for easy comparison\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Ensemble Model Evaluation:\")\n",
        "# Ensure ensemble_classifier and test_ensemble_features_selected are available\n",
        "if 'ensemble_classifier' in locals() and 'test_ensemble_features_selected' in locals() and test_ensemble_features_selected is not None:\n",
        "     ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "     print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "     print(\"Confusion Matrix:\")\n",
        "     print(confusion_matrix(test_labels_for_model, ensemble_predictions))\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c161b79"
      },
      "source": [
        "### Comparative Analysis\n",
        "\n",
        "Based on the classification reports and confusion matrices, we can compare the performance of the three models, paying close attention to metrics like precision, recall, and F1-score for the \"damaged\" class, given the class imbalance.\n",
        "\n",
        "*   **Random Forest Classifier**: [Summarize key findings from the RF evaluation]\n",
        "*   **Convolutional Neural Network (CNN)**: [Summarize key findings from the CNN evaluation]\n",
        "*   **Ensemble Model**: [Summarize key findings from the Ensemble model evaluation]\n",
        "\n",
        "Consider which model achieved the best balance of precision and recall for the damaged nuclei, and discuss any insights gained from the feature importance analysis of the Random Forest model.\n",
        "\n",
        "This concludes the analysis of the trained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f927a7b8"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    # Reshape texture features to be 2D if they are 3D (as seen in the output)\n",
        "    # The shape (30, 4, 13) suggests 4 directions with 13 features each.\n",
        "    # We can flatten the last two dimensions to get (30, 52)\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    # Reshape texture features similarly\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd42e68d"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Apply SMOTE to handle class imbalance\n",
        "smote = SMOTE(random_state=42)\n",
        "train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "# Instantiate and train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "print(\"Random Forest classifier trained successfully with SMOTE.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03f9e2dd"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = rf_classifier.predict(test_combined_features)\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The original texture features shape was (samples, num_features_per_image)\n",
        "# After flattening, it should be (samples, num_features_per_image)\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "# Assuming train_texture_features_flat was successfully created:\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created due to the previous error,\n",
        "    # we need to recreate it or calculate its expected size.\n",
        "    # Based on the previous output (30, 76), the flattened size should be 76.\n",
        "    # We need to update this to reflect the *new* number of flattened texture features\n",
        "    # after incorporating the additional features.\n",
        "    # Based on the shape of train_combined_features (30, 251) and train_shape_features (30, 159),\n",
        "    # the number of flattened texture features is 251 - 159 = 92.\n",
        "    num_texture_features_flat = 92 # Manually set based on new combined feature shape\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c41a9c9"
      },
      "source": [
        "## Train convolutional neural network (cnn)\n",
        "\n",
        "### Subtask:\n",
        "Build and train a CNN directly on the normalized nucleus images, incorporating data augmentation, regularization, and potentially transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d2fad12"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the CNN model architecture, compile it, set up data augmentation, and train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4312bc0a"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model)\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model))\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ad0583f"
      },
      "source": [
        "## Evaluate convolutional neural network (cnn)\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f15593e6"
      },
      "source": [
        "**Reasoning**:\n",
        "Utilize the trained CNN model to generate predictions on the test set, convert probabilities to class labels, and then generate and print the classification report and confusion matrix to evaluate the model's performance as requested by the subtask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b490501"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd8d3c48"
      },
      "source": [
        "## Build and Train Ensemble Model\n",
        "\n",
        "### Subtask:\n",
        "Combine the CNN probabilities with the handcrafted features and train a second-level Random Forest model, including feature selection and optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28062b74"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the predictions (probabilities) from the trained CNN with the extracted shape and texture features, and then train a second-level Random Forest classifier on this combined feature set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07529219"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model)\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model)\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for combined features and CNN predictions.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c5fd83f"
      },
      "source": [
        "## Evaluate Ensemble Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the ensemble model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "593bc73c"
      },
      "source": [
        "**Reasoning**:\n",
        "Utilize the trained ensemble model to generate predictions on the test set and then generate and print the classification report and confusion matrix to evaluate its performance as requested by the subtask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d45f5835"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ea433d"
      },
      "source": [
        "## Summary of Model Performance\n",
        "\n",
        "We have trained and evaluated three different models for classifying damaged nuclei: a Random Forest classifier using handcrafted features, a Convolutional Neural Network (CNN) trained on normalized images, and an ensemble model combining CNN probabilities and handcrafted features.\n",
        "\n",
        "Here's a summary of the key performance metrics for each model on the test set:\n",
        "\n",
        "### Random Forest Classifier (using handcrafted features with SMOTE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14cfeac2"
      },
      "source": [
        "# Print the classification report and confusion matrix for the Random Forest model again for easy comparison\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3458f1f"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "523dbea3"
      },
      "source": [
        "# Print the classification report and confusion matrix for the CNN model again for easy comparison\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"CNN Evaluation:\")\n",
        "# Ensure model and test_images_for_model are available from previous steps\n",
        "if 'model' in locals() and 'test_images_for_model' in locals():\n",
        "    predictions_prob_cnn = model.predict(test_images_for_model)\n",
        "    predictions_cnn = (predictions_prob_cnn > 0.5).astype(\"int32\")\n",
        "    print(classification_report(test_labels_for_model, predictions_cnn))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(test_labels_for_model, predictions_cnn))\n",
        "else:\n",
        "    print(\"CNN model or test data not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbd2d32c"
      },
      "source": [
        "### Ensemble Model (CNN probabilities + Handcrafted Features with SMOTE and Feature Selection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c7cae9a"
      },
      "source": [
        "# Print the classification report and confusion matrix for the Ensemble model again for easy comparison\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Ensemble Model Evaluation:\")\n",
        "# Ensure ensemble_classifier and test_ensemble_features_selected are available\n",
        "if 'ensemble_classifier' in locals() and 'test_ensemble_features_selected' in locals() and test_ensemble_features_selected is not None:\n",
        "     ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "     print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "     print(\"Confusion Matrix:\")\n",
        "     print(confusion_matrix(test_labels_for_model, ensemble_predictions))\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b929e1a"
      },
      "source": [
        "### Comparative Analysis\n",
        "\n",
        "Based on the classification reports and confusion matrices, we can compare the performance of the three models, paying close attention to metrics like precision, recall, and F1-score for the \"damaged\" class, given the class imbalance.\n",
        "\n",
        "*   **Random Forest Classifier**: [Summarize key findings from the RF evaluation]\n",
        "*   **Convolutional Neural Network (CNN)**: [Summarize key findings from the CNN evaluation]\n",
        "*   **Ensemble Model**: [Summarize key findings from the Ensemble model evaluation]\n",
        "\n",
        "Consider which model achieved the best balance of precision and recall for the damaged nuclei, and discuss any insights gained from the feature importance analysis of the Random Forest model.\n",
        "\n",
        "This concludes the analysis of the trained models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3559360"
      },
      "source": [
        "### Refine Green Channel Thresholding\n",
        "\n",
        "Let's experiment with adjusting the global threshold value for the green channel to see if it improves foci detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27d990a5"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Example: Apply thresholding with a different value (e.g., 30) to one of the training images\n",
        "# and display the result to visually assess the effect.\n",
        "example_image_index = 0 # You can change this index to view other images\n",
        "threshold_value = 30   # Experiment with this threshold value\n",
        "\n",
        "example_green_image = train_green_images[example_image_index]\n",
        "thresholded_example_green = apply_global_thresholding(example_green_image, threshold_value)\n",
        "\n",
        "print(f\"Applied global thresholding to training green image at index {example_image_index} with threshold value {threshold_value}.\")\n",
        "\n",
        "# Display the original and thresholded images for comparison\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "print(\"Original Green Image:\")\n",
        "cv2_imshow(example_green_image)\n",
        "\n",
        "print(f\"\\nThresholded Green Image (Threshold = {threshold_value}):\")\n",
        "cv2_imshow(thresholded_example_green)\n",
        "\n",
        "# Note: We will apply this thresholding to all images after finding a suitable value."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7bd9a62"
      },
      "source": [
        "# Apply the chosen thresholding to all training and testing green images\n",
        "# Using the threshold_value from the previous cell\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")\n",
        "\n",
        "# Re-run foci detection and nucleus labeling with the new thresholded images\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei with the new labeling\n",
        "train_damaged_count_new = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count_new = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count_new = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count_new = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(\"\\nNuclei counts with updated green channel thresholding:\")\n",
        "print(f\"Training set: {train_damaged_count_new} damaged, {train_normal_count_new} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count_new} damaged, {test_normal_count_new} normal.\")\n",
        "\n",
        "# Update the all_train_labeled_nuclei and all_test_labeled_nuclei variables\n",
        "all_train_labeled_nuclei = train_labeled_nuclei\n",
        "all_test_labeled_nuclei = test_labeled_nuclei"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63183b6d"
      },
      "source": [
        "## Optimize Random Forest Classifier\n",
        "\n",
        "### Subtask:\n",
        "Optimize the Random Forest model by experimenting with hyperparameters and class weighting to improve performance on the imbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7440c8f1"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Update the combined features and labels with the newly processed data\n",
        "# Assuming all_train_labeled_nuclei and all_test_labeled_nuclei have been updated\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension (if not already done)\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format (if not already done)\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Re-extract and combine features with the potentially updated labels\n",
        "# This ensures the features correspond to the new labeling\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_labeled_nuclei] # Corrected typo here\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "# Ensure all texture feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "# Re-combine shape and texture features\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in training samples for shape and texture features after re-extraction.\")\n",
        "    train_combined_features = None\n",
        "\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in testing samples for shape and texture features after re-extraction.\")\n",
        "    test_combined_features = None\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e37a28b"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bae073c4"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Update the combined features and labels with the newly processed data\n",
        "# Assuming all_train_labeled_nuclei and all_test_labeled_nuclei have been updated\n",
        "train_images_for_model_re = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model_re = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "test_images_for_model_re = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model_re = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension (if not already done)\n",
        "train_images_for_model_re = np.array(train_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model_re = np.array(test_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format (if not already done)\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model_re = np.array([label_mapping[label] for label in train_labels_for_model_re])\n",
        "test_labels_for_model_re = np.array([label_mapping[label] for label in test_labels_for_model_re])\n",
        "\n",
        "# Re-extract and combine features with the potentially updated labels\n",
        "# This ensures the features correspond to the new labeling\n",
        "train_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "train_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features_re = np.array(train_shape_features_re)\n",
        "test_shape_features_re = np.array(test_shape_features_re)\n",
        "\n",
        "# Ensure all texture feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features_re = pad_features(train_texture_features_re)\n",
        "test_texture_features_re = pad_features(test_texture_features_re)\n",
        "\n",
        "# Re-combine shape and texture features\n",
        "if train_shape_features_re.shape[0] == train_texture_features_re.shape[0]:\n",
        "    train_texture_features_flat_re = train_texture_features_re.reshape(train_texture_features_re.shape[0], -1)\n",
        "    train_combined_features_re = np.concatenate((train_shape_features_re, train_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in training samples for shape and texture features after re-extraction.\")\n",
        "    train_combined_features_re = None\n",
        "\n",
        "if test_shape_features_re.shape[0] == test_texture_features_re.shape[0]:\n",
        "    test_texture_features_flat_re = test_texture_features_re.reshape(test_texture_features_re.shape[0], -1)\n",
        "    test_combined_features_re = np.concatenate((test_shape_features_re, test_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in testing samples for shape and texture features after re-extraction.\")\n",
        "    test_combined_features_re = None\n",
        "\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features_re is not None and train_labels_for_model_re is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features_re, train_labels_for_model_re)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features_re is not None and test_labels_for_model_re is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features_re)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model_re, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "781d1098"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84080fd0"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Update the combined features and labels with the newly processed data\n",
        "# Assuming all_train_labeled_nuclei and all_test_labeled_nuclei have been updated\n",
        "train_images_for_model_re = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model_re = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "test_images_for_model_re = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model_re = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension (if not already done)\n",
        "train_images_for_model_re = np.array(train_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model_re = np.array(test_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format (if not already done)\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model_re = np.array([label_mapping[label] for label in train_labels_for_model_re])\n",
        "test_labels_for_model_re = np.array([label_mapping[label] for label in test_labels_for_model_re])\n",
        "\n",
        "# Re-extract and combine features with the potentially updated labels\n",
        "# This ensures the features correspond to the new labeling\n",
        "train_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "train_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features_re = np.array(train_shape_features_re)\n",
        "test_shape_features_re = np.array(test_shape_features_re)\n",
        "\n",
        "# Ensure all texture feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features_re = pad_features(train_texture_features_re)\n",
        "test_texture_features_re = pad_features(test_texture_features_re)\n",
        "\n",
        "# Re-combine shape and texture features\n",
        "if train_shape_features_re.shape[0] == train_texture_features_re.shape[0]:\n",
        "    train_texture_features_flat_re = train_texture_features_re.reshape(train_texture_features_re.shape[0], -1)\n",
        "    train_combined_features_re = np.concatenate((train_shape_features_re, train_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in training samples for shape and texture features after re-extraction.\")\n",
        "    train_combined_features_re = None\n",
        "\n",
        "if test_shape_features_re.shape[0] == test_texture_features_re.shape[0]:\n",
        "    test_texture_features_flat_re = test_texture_features_re.reshape(test_texture_features_re.shape[0], -1)\n",
        "    test_combined_features_re = np.concatenate((test_shape_features_re, test_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in testing samples for shape and texture features after re-extraction.\")\n",
        "    test_combined_features_re = None\n",
        "\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features_re is not None and train_labels_for_model_re is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features_re, train_labels_for_model_re)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features_re is not None and test_labels_for_model_re is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features_re)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model_re, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3bdcda2"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f7bb906"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7235e39d"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56fcc7ff"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26078344"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Update the combined features and labels with the newly processed data\n",
        "# Assuming all_train_labeled_nuclei and all_test_labeled_nuclei have been updated\n",
        "train_images_for_model_re = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model_re = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "test_images_for_model_re = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model_re = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension (if not already done)\n",
        "train_images_for_model_re = np.array(train_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model_re = np.array(test_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format (if not already done)\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model_re = np.array([label_mapping[label] for label in train_labels_for_model_re])\n",
        "test_labels_for_model_re = np.array([label_mapping[label] for label in test_labels_for_model_re])\n",
        "\n",
        "# Re-extract and combine features with the potentially updated labels\n",
        "# This ensures the features correspond to the new labeling\n",
        "train_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "train_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features_re = np.array(train_shape_features_re)\n",
        "test_shape_features_re = np.array(test_shape_features_re)\n",
        "\n",
        "# Ensure all texture feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features_re = pad_features(train_texture_features_re)\n",
        "test_texture_features_re = pad_features(test_texture_features_re)\n",
        "\n",
        "# Re-combine shape and texture features\n",
        "if train_shape_features_re.shape[0] == train_texture_features_re.shape[0]:\n",
        "    train_texture_features_flat_re = train_texture_features_re.reshape(train_texture_features_re.shape[0], -1)\n",
        "    train_combined_features_re = np.concatenate((train_shape_features_re, train_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in training samples for shape and texture features after re-extraction.\")\n",
        "    train_combined_features_re = None\n",
        "\n",
        "if test_shape_features_re.shape[0] == test_texture_features_re.shape[0]:\n",
        "    test_texture_features_flat_re = test_texture_features_re.reshape(test_texture_features_re.shape[0], -1)\n",
        "    test_combined_features_re = np.concatenate((test_shape_features_re, test_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in testing samples for shape and texture features after re-extraction.\")\n",
        "    test_combined_features_re = None\n",
        "\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features_re is not None and train_labels_for_model_re is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features_re, train_labels_for_model_re)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features_re is not None and test_labels_for_model_re is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features_re)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model_re, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eeebcdc"
      },
      "source": [
        "!unzip DNA_Damage_Project-20250901T154313Z-1-001.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73520c9c"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba636a77"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59752fa1"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4746e4e1"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "116a753c"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19565b88"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08cf1a61"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7efce5b5"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Update the combined features and labels with the newly processed data\n",
        "# Assuming all_train_labeled_nuclei and all_test_labeled_nuclei have been updated\n",
        "train_images_for_model_re = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model_re = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "test_images_for_model_re = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model_re = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension (if not already done)\n",
        "train_images_for_model_re = np.array(train_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model_re = np.array(test_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format (if not already done)\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model_re = np.array([label_mapping[label] for label in train_labels_for_model_re])\n",
        "test_labels_for_model_re = np.array([label_mapping[label] for label in test_labels_for_model_re])\n",
        "\n",
        "# Re-extract and combine features with the potentially updated labels\n",
        "# This ensures the features correspond to the new labeling\n",
        "train_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "train_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features_re = np.array(train_shape_features_re)\n",
        "test_shape_features_re = np.array(test_texture_features_re) # Corrected: should be test_texture_features_re\n",
        "\n",
        "# Ensure all texture feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features_re = pad_features(train_texture_features_re)\n",
        "test_texture_features_re = pad_features(test_texture_features_re)\n",
        "\n",
        "# Re-combine shape and texture features\n",
        "if train_shape_features_re.shape[0] == train_texture_features_re.shape[0]:\n",
        "    train_texture_features_flat_re = train_texture_features_re.reshape(train_texture_features_re.shape[0], -1)\n",
        "    train_combined_features_re = np.concatenate((train_shape_features_re, train_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in training samples for shape and texture features after re-extraction.\")\n",
        "    train_combined_features_re = None\n",
        "\n",
        "if test_shape_features_re.shape[0] == test_texture_features_re.shape[0]:\n",
        "    test_texture_features_flat_re = test_texture_features_re.reshape(test_texture_features_re.shape[0], -1)\n",
        "    test_combined_features_re = np.concatenate((test_shape_features_re, test_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in testing samples for shape and texture features after re-extraction.\")\n",
        "    test_combined_features_re = None\n",
        "\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features_re is not None and train_labels_for_model_re is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features_re, train_labels_for_model_re)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features_re is not None and test_labels_for_model_re is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features_re)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model_re, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcdf970a"
      },
      "source": [
        "!ls DNA_Damage_Project"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4d5f1d8"
      },
      "source": [
        "!unzip -l DNA_Damage_Project-20250901T154313Z-1-001.zip\n",
        "!unzip DNA_Damage_Project-20250901T154313Z-1-001.zip -d ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63030878"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86292a14"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcf7ed7f"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7962f544"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38530e8a"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ad6008c6"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de4b1196"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d272e13"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Update the combined features and labels with the newly processed data\n",
        "# Assuming all_train_labeled_nuclei and all_test_labeled_nuclei have been updated\n",
        "train_images_for_model_re = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model_re = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "test_images_for_model_re = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model_re = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension (if not already done)\n",
        "train_images_for_model_re = np.array(train_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model_re = np.array(test_images_for_model_re).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format (if not already done)\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model_re = np.array([label_mapping[label] for label in train_labels_for_model_re])\n",
        "test_labels_for_model_re = np.array([label_mapping[label] for label in test_labels_for_model_re])\n",
        "\n",
        "# Re-extract and combine features with the potentially updated labels\n",
        "# This ensures the features correspond to the new labeling\n",
        "train_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features_re = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "train_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features_re = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features_re = np.array(train_shape_features_re)\n",
        "test_shape_features_re = np.array(test_shape_features_re)\n",
        "\n",
        "# Ensure all texture feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features_re = pad_features(train_texture_features_re)\n",
        "test_texture_features_re = pad_features(test_texture_features_re)\n",
        "\n",
        "# Re-combine shape and texture features\n",
        "if train_shape_features_re.shape[0] == train_texture_features_re.shape[0]:\n",
        "    train_texture_features_flat_re = train_texture_features_re.reshape(train_texture_features_re.shape[0], -1)\n",
        "    train_combined_features_re = np.concatenate((train_shape_features_re, train_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in training samples for shape and texture features after re-extraction.\")\n",
        "    train_combined_features_re = None\n",
        "\n",
        "if test_shape_features_re.shape[0] == test_texture_features_re.shape[0]:\n",
        "    test_texture_features_flat_re = test_texture_features_re.reshape(test_texture_features_re.shape[0], -1)\n",
        "    test_combined_features_re = np.concatenate((test_shape_features_re, test_texture_features_flat_re), axis=1)\n",
        "else:\n",
        "    print(\"Mismatch in testing samples for shape and texture features after re-extraction.\")\n",
        "    test_combined_features_re = None\n",
        "\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features_re is not None and train_labels_for_model_re is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features_re, train_labels_for_model_re)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 1, 4], # Corrected typo here\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features_re is not None and test_labels_for_model_re is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features_re)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model_re, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93c22a6c"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model_re) # Use the re-extracted images\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model_re, train_labels_for_model_re, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model_re, test_labels_for_model_re)) # Use the re-extracted images and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9817b7e5"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model_re) # Use the re-extracted images\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model_re, predictions)) # Use the re-extracted labels\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model_re, predictions) # Use the re-extracted labels\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce46ff7c"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model_re) # Use the re-extracted images\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model_re) # Use the re-extracted images\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features_re.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features_re = np.concatenate((train_combined_features_re, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features_re: {train_ensemble_features_re.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features_re = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features_re.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features_re = np.concatenate((test_combined_features_re, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features_re: {test_ensemble_features_re.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for combined features and CNN predictions.\")\n",
        "    test_ensemble_features_re = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features_re is not None and train_labels_for_model_re is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean_re = np.nan_to_num(train_ensemble_features_re)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean_re.shape[0] > 1 and np.var(train_ensemble_features_clean_re, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean_re.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected_re = selector.fit_transform(train_ensemble_features_clean_re, train_labels_for_model_re)\n",
        "            test_ensemble_features_selected_re = selector.transform(np.nan_to_num(test_ensemble_features_re)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected_re: {train_ensemble_features_selected_re.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected_re: {test_ensemble_features_selected_re.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected_re = train_ensemble_features_clean_re\n",
        "             test_ensemble_features_selected_re = np.nan_to_num(test_ensemble_features_re)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected_re = train_ensemble_features_clean_re\n",
        "         test_ensemble_features_selected_re = np.nan_to_num(test_ensemble_features_re)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected_re = None\n",
        "    test_ensemble_features_selected_re = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected_re is not None and train_labels_for_model_re is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote_re, train_labels_ensemble_smote_re = smote_ensemble.fit_resample(train_ensemble_features_selected_re, train_labels_for_model_re)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier_re = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier_re.fit(train_ensemble_features_smote_re, train_labels_ensemble_smote_re)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier_re = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c562a0a"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected_re is not None and ensemble_classifier_re is not None:\n",
        "    ensemble_predictions_re = ensemble_classifier_re.predict(test_ensemble_features_selected_re)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model_re, ensemble_predictions_re))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy_re = accuracy_score(test_labels_for_model_re, ensemble_predictions_re)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy_re}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble_re = confusion_matrix(test_labels_for_model_re, ensemble_predictions_re)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble_re)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "306bb7ac"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77274d13"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34e67bf6"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84c06fb7"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "331e09c1"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d41cff85"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9431cd8"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa6f7608"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eed15d26"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4943fba4"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2 + 4*4) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) + skimage haralick\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d5728b3"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd55cf08"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a995347"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_rf_classifier.predict(test_combined_features) # Use the best classifier\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Optimized Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances from the best classifier\n",
        "feature_importances = best_rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created, calculate its expected size\n",
        "    # based on the total combined features and shape features\n",
        "    if train_combined_features is not None and train_shape_features is not None:\n",
        "        num_texture_features_flat = train_combined_features.shape[1] - train_shape_features.shape[1]\n",
        "    else:\n",
        "        # Fallback if necessary variables are not defined\n",
        "        num_texture_features_flat = 0 # Default to 0 if cannot determine\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16e876da"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model) # Use the original train_images_for_model\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model)) # Use the original test_images_for_model and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daee2210"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a109cc7"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model) # Use the original train_images_for_model\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model) # Use the original test_images_for_model\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for combined features and CNN predictions.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "131ef2db"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "428a3e30"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55c78ca9"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48def425"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79e5c0be"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33072124"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5626c753"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25bea2e3"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c57e7992"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb5d6d63"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b2b2eea"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2 + 4*4) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) + skimage haralick\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44a56f5a"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8428d3f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04c0acfb"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_rf_classifier.predict(test_combined_features) # Use the best classifier\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Optimized Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances from the best classifier\n",
        "feature_importances = best_rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created, calculate its expected size\n",
        "    # based on the total combined features and shape features\n",
        "    if train_combined_features is not None and train_shape_features is not None:\n",
        "        num_texture_features_flat = train_combined_features.shape[1] - train_shape_features.shape[1]\n",
        "    else:\n",
        "        # Fallback if necessary variables are not defined\n",
        "        num_texture_features_flat = 0 # Default to 0 if cannot determine\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d6ff218"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model) # Use the original train_images_for_model\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model)) # Use the original test_images_for_model and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c419d94b"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c21e63f"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model) # Use the original train_images_for_model\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model) # Use the original test_images_for_model\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for combined features and CNN predictions.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a744d1c"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d602303"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee4c8abb"
      },
      "source": [
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51d2ae36"
      },
      "source": [
        "!unzip '/content/DNA_Damage_Project-20250901T154313Z-1-001 (1).zip' -d ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d165045"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Swap train and test assignments based on the user's feedback\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baefee08"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Swap train and test assignments based on the user's feedback\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a623233d"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "# Swap train and test paired images assignments based on the user's feedback\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24e27b2c"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbcb05a5"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3e8efd7"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6aef8e1"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43f4889c"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "287c61a8"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5933da4d"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2 + 4*4) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) + skimage haralick\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aa0451b"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d26e7f29"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d451277d"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_rf_classifier.predict(test_combined_features) # Use the best classifier\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Optimized Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances from the best classifier\n",
        "feature_importances = best_rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created, calculate its expected size\n",
        "    # based on the total combined features and shape features\n",
        "    if train_combined_features is not None and train_shape_features is not None:\n",
        "        num_texture_features_flat = train_combined_features.shape[1] - train_shape_features.shape[1]\n",
        "    else:\n",
        "        # Fallback if necessary variables are not defined\n",
        "        num_texture_features_flat = 0 # Default to 0 if cannot determine\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24577f41"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model) # Use the original train_images_for_model\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model)) # Use the original test_images_for_model and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ed40971"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be40a999"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model) # Use the original train_images_for_model\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model) # Use the original test_images_for_model\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ad91e19"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d546bea7"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f99457c6"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcf1093d"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff5111de"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d38bdbd"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ea77c6"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e649b93d"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2 + 4*4) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) + skimage haralick\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39211d9b"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeb6383b"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9708954"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_rf_classifier.predict(test_combined_features) # Use the best classifier\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Optimized Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances from the best classifier\n",
        "feature_importances = best_rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created, calculate its expected size\n",
        "    # based on the total combined features and shape features\n",
        "    if train_combined_features is not None and train_shape_features is not None:\n",
        "        num_texture_features_flat = train_combined_features.shape[1] - train_shape_features.shape[1]\n",
        "    else:\n",
        "        # Fallback if necessary variables are not defined\n",
        "        num_texture_features_flat = 0 # Default to 0 if cannot determine\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98a990b2"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model) # Use the original train_images_for_model\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model)) # Use the original test_images_for_model and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dceda4f"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5351a10"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model) # Use the original train_images_for_model\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model) # Use the original test_images_for_model\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e9a4d00"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e088b79"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Swap train and test assignments based on the user's feedback\n",
        "train_red_images = load_and_preprocess_images('DNA_Damage_Project/test images/red')\n",
        "test_red_images = load_and_preprocess_images('DNA_Damage_Project/image/red')\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(train_red_images)} training red images.\")\n",
        "print(f\"Loaded {len(test_red_images)} testing red images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80621d2d"
      },
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_images(base_dir):\n",
        "    \"\"\"Loads images from specified directory and converts to grayscale.\"\"\"\n",
        "    images = []\n",
        "    image_filenames = sorted(os.listdir(base_dir))\n",
        "    for filename in image_filenames:\n",
        "        img_path = os.path.join(base_dir, filename)\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if image was loaded successfully\n",
        "        if img is None:\n",
        "            print(f\"Warning: Could not load image {img_path}\")\n",
        "            continue\n",
        "\n",
        "        # OpenCV loads images in BGR format, convert to grayscale\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        images.append(gray_img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Swap train and test assignments based on the user's feedback\n",
        "train_green_images = load_and_preprocess_images('DNA_Damage_Project/test images/green')\n",
        "test_green_images = load_and_preprocess_images('DNA_Damage_Project/image/green')\n",
        "\n",
        "\n",
        "print(f\"Loaded {len(train_green_images)} training green images.\")\n",
        "print(f\"Loaded {len(test_green_images)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f1dc7f5"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "# Swap train and test paired images assignments based on the user's feedback\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ede5e5f5"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7e232014"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4f49d528"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9036f903"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9daf0332"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b047db3"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb6ff2a6"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2 + 4*4) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) + skimage haralick\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9844adf6"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3696bfa8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e048a768"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_rf_classifier.predict(test_combined_features) # Use the best classifier\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Optimized Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances from the best classifier\n",
        "feature_importances = best_rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created, calculate its expected size\n",
        "    # based on the total combined features and shape features\n",
        "    if train_combined_features is not None and train_shape_features is not None:\n",
        "        num_texture_features_flat = train_combined_features.shape[1] - train_shape_features.shape[1]\n",
        "    else:\n",
        "        # Fallback if necessary variables are not defined\n",
        "        num_texture_features_flat = 0 # Default to 0 if cannot determine\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a814f81"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model) # Use the original train_images_for_model\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model)) # Use the original test_images_for_model and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8bf6eda"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ac4f03"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model) # Use the original train_images_for_model\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model) # Use the original test_images_for_model\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a66c70d"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da4437fc"
      },
      "source": [
        "def pair_images(red_images, green_images):\n",
        "    \"\"\"Pairs corresponding red and green images.\"\"\"\n",
        "    # Assuming images are already sorted by filename in the loading step\n",
        "    # and that the filenames match between red and green directories\n",
        "    paired_images = []\n",
        "    for i in range(len(red_images)):\n",
        "        paired_images.append((red_images[i], green_images[i]))\n",
        "    return paired_images\n",
        "\n",
        "# Swap train and test paired images assignments based on the user's feedback\n",
        "train_paired_images = pair_images(train_red_images, train_green_images)\n",
        "test_paired_images = pair_images(test_red_images, test_green_images)\n",
        "\n",
        "\n",
        "print(f\"Created {len(train_paired_images)} paired training images.\")\n",
        "print(f\"Created {len(test_paired_images)} paired testing images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1d1b1ce"
      },
      "source": [
        "def segment_nuclei(image):\n",
        "    \"\"\"Segments nuclei using Otsu's thresholding and watershed.\"\"\"\n",
        "    # Apply Otsu's thresholding\n",
        "    _, thresh = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Morphological operations to clean up the mask\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "    # Sure background area\n",
        "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "    # Sure foreground area\n",
        "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "    _, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "    # Unknown region\n",
        "    sure_fg = np.uint8(sure_fg)\n",
        "    unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "    # Marker labelling\n",
        "    _, markers = cv2.connectedComponents(sure_fg)\n",
        "\n",
        "    # Add one to all labels so that sure background is not 0, but 1\n",
        "    markers = markers + 1\n",
        "\n",
        "    # Now, mark the region of unknown with zero\n",
        "    markers[unknown == 255] = 0\n",
        "\n",
        "    # Apply watershed algorithm\n",
        "    # Create a BGR version of the grayscale image for watershed visualization (optional, but watershed expects 3 channels)\n",
        "    img_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "    markers = cv2.watershed(img_bgr, markers)\n",
        "\n",
        "    # Create a binary mask from the watershed result\n",
        "    segmented_mask = np.zeros_like(image, dtype=np.uint8)\n",
        "    segmented_mask[markers > 1] = 255  # Nuclei are marked with labels > 1\n",
        "\n",
        "    # Remove small objects (noise)\n",
        "    min_size = 100  # Minimum size threshold for nuclei\n",
        "    num_labels, labels_img, stats, centroids = cv2.connectedComponentsWithStats(segmented_mask, connectivity=8)\n",
        "\n",
        "    cleaned_mask = np.zeros_like(segmented_mask)\n",
        "    for i in range(1, num_labels): # Start from 1 to exclude background\n",
        "        if stats[i, cv2.CC_STAT_AREA] >= min_size:\n",
        "            cleaned_mask[labels_img == i] = 255\n",
        "\n",
        "    return cleaned_mask\n",
        "\n",
        "train_segmented_masks = [segment_nuclei(img) for img in train_red_images]\n",
        "test_segmented_masks = [segment_nuclei(img) for img in test_red_images]\n",
        "\n",
        "print(f\"Generated {len(train_segmented_masks)} training segmented masks.\")\n",
        "print(f\"Generated {len(test_segmented_masks)} testing segmented masks.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dec3543a"
      },
      "source": [
        "def apply_global_thresholding(image, threshold_value=50):\n",
        "    \"\"\"Applies global thresholding to a grayscale image.\"\"\"\n",
        "    # Apply global thresholding\n",
        "    _, thresholded_img = cv2.threshold(image, threshold_value, 255, cv2.THRESH_BINARY)\n",
        "    return thresholded_img\n",
        "\n",
        "# Apply thresholding to training and testing green images\n",
        "# Using the threshold_value from the previous cell if it exists, otherwise use a default\n",
        "global threshold_value\n",
        "try:\n",
        "    threshold_value_to_use = threshold_value\n",
        "except NameError:\n",
        "    threshold_value_to_use = 50 # Default value\n",
        "\n",
        "train_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in train_green_images]\n",
        "test_green_thresholded = [apply_global_thresholding(img, threshold_value_to_use) for img in test_green_images]\n",
        "\n",
        "print(f\"Applied global thresholding to {len(train_green_thresholded)} training green images.\")\n",
        "print(f\"Applied global thresholding to {len(test_green_thresholded)} testing green images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "442cb183"
      },
      "source": [
        "def detect_foci_and_label_nucleus(paired_image, segmented_mask, thresholded_green_image, foci_threshold=10):\n",
        "    \"\"\"\n",
        "    Detects green foci within segmented nuclei and labels each nucleus\n",
        "    as \"damaged\" or \"normal\".\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "        segmented_mask: The binary segmented mask for the nuclei.\n",
        "        thresholded_green_image: The thresholded green channel image.\n",
        "        foci_threshold: The minimum number of non-zero pixels (foci) to label a nucleus as \"damaged\".\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "    labeled_nuclei = []\n",
        "\n",
        "    # Find contours in the segmented mask to identify individual nuclei\n",
        "    contours, _ = cv2.findContours(segmented_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "        # Create a mask specifically for the current nucleus\n",
        "        nucleus_mask = np.zeros_like(segmented_mask)\n",
        "        cv2.drawContours(nucleus_mask, [contour], -1, 255, -1)\n",
        "\n",
        "        # Apply this nucleus mask to the thresholded green channel image\n",
        "        masked_green = cv2.bitwise_and(thresholded_green_image, thresholded_green_image, mask=nucleus_mask)\n",
        "\n",
        "        # Count the number of non-zero pixels (representing potential foci)\n",
        "        foci_count = np.count_nonzero(masked_green)\n",
        "\n",
        "        # Determine the label based on foci count\n",
        "        label = \"damaged\" if foci_count > foci_threshold else \"normal\"\n",
        "\n",
        "        # Find bounding box\n",
        "        x, y, w, h = cv2.boundingRect(contour)\n",
        "\n",
        "        # Extract ROI for the red channel\n",
        "        red_roi = red_img[y:y+h, x:x+w]\n",
        "\n",
        "        # Calculate minimum enclosing circle\n",
        "        (center_x, center_y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center_x -= x  # Adjust center to be relative to ROI\n",
        "        center_y -= y\n",
        "\n",
        "        # Determine size of square bounding box for the circle\n",
        "        square_size = int(np.ceil(2 * radius))\n",
        "        padding = square_size\n",
        "\n",
        "        # Pad the red ROI to the square size, centering the nucleus\n",
        "        # Calculate padding amounts\n",
        "        pad_x_before = int((padding - w) / 2)\n",
        "        pad_x_after = padding - w - pad_x_before\n",
        "        pad_y_before = int((padding - h) / 2)\n",
        "        pad_y_after = padding - h - pad_y_before\n",
        "\n",
        "        padded_red_roi = np.pad(red_roi, ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), mode='constant')\n",
        "\n",
        "        # Resize to target size (assuming the target size is 64x64 as used in the previous normalization step)\n",
        "        target_size = (64, 64)\n",
        "        normalized_red_nucleus = cv2.resize(padded_red_roi, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "        # Store the normalized red nucleus image and its label\n",
        "        labeled_nuclei.append((normalized_red_nucleus, label))\n",
        "\n",
        "    return labeled_nuclei\n",
        "\n",
        "# Apply foci detection and labeling to training and testing sets\n",
        "train_labeled_nuclei = []\n",
        "for i in range(len(train_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        train_paired_images[i],\n",
        "        train_segmented_masks[i],\n",
        "        train_green_thresholded[i]\n",
        "    )\n",
        "    train_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "test_labeled_nuclei = []\n",
        "for i in range(len(test_paired_images)):\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        test_paired_images[i],\n",
        "        test_segmented_masks[i],\n",
        "        test_green_thresholded[i]\n",
        "    )\n",
        "    test_labeled_nuclei.extend(labeled_nuclei_in_image)\n",
        "\n",
        "# Count damaged and normal nuclei\n",
        "train_damaged_count = sum(1 for img, label in train_labeled_nuclei if label == \"damaged\")\n",
        "train_normal_count = sum(1 for img, label in train_labeled_nuclei if label == \"normal\")\n",
        "test_damaged_count = sum(1 for img, label in test_labeled_nuclei if label == \"damaged\")\n",
        "test_normal_count = sum(1 for img, label in test_labeled_nuclei if label == \"normal\")\n",
        "\n",
        "print(f\"Training set: {train_damaged_count} damaged, {train_normal_count} normal.\")\n",
        "print(f\"Testing set: {test_damaged_count} damaged, {test_normal_count} normal.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3004cdc5"
      },
      "source": [
        "def process_image_pair(paired_image):\n",
        "    \"\"\"\n",
        "    Processes a paired image (red, green) through segmentation,\n",
        "    thresholding, foci detection, and labeling.\n",
        "\n",
        "    Args:\n",
        "        paired_image: A tuple containing the red and green channel images (grayscale).\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple contains the normalized\n",
        "        red nucleus image and its corresponding label (\"damaged\" or \"normal\").\n",
        "    \"\"\"\n",
        "    red_img, green_img = paired_image\n",
        "\n",
        "    # 1. Nucleus segmentation (red channel)\n",
        "    segmented_mask = segment_nuclei(red_img)\n",
        "\n",
        "    # 2. Green channel global thresholding\n",
        "    # Use the threshold_value defined previously (or a default if not defined)\n",
        "    global threshold_value # Access the threshold value from the previous cell if it exists\n",
        "    try:\n",
        "        threshold_value_to_use = threshold_value\n",
        "    except NameError:\n",
        "        threshold_value_to_use = 50 # Default value if not set\n",
        "\n",
        "    thresholded_green_image = apply_global_thresholding(green_img, threshold_value_to_use)\n",
        "\n",
        "\n",
        "    # 3. Foci detection and nucleus labeling\n",
        "    labeled_nuclei_in_image = detect_foci_and_label_nucleus(\n",
        "        paired_image,\n",
        "        segmented_mask,\n",
        "        thresholded_green_image\n",
        "    )\n",
        "\n",
        "    return labeled_nuclei_in_image\n",
        "\n",
        "# Process training images\n",
        "all_train_labeled_nuclei = []\n",
        "for paired_img in train_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_train_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Process testing images\n",
        "all_test_labeled_nuclei = []\n",
        "for paired_img in test_paired_images:\n",
        "    labeled_nuclei = process_image_pair(paired_img)\n",
        "    all_test_labeled_nuclei.extend(labeled_nuclei)\n",
        "\n",
        "# Print the total number of labeled nuclei\n",
        "print(f\"Total labeled nuclei in training set: {len(all_train_labeled_nuclei)}\")\n",
        "print(f\"Total labeled nuclei in testing set: {len(all_test_labeled_nuclei)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f881a894"
      },
      "source": [
        "# Separate images and labels for training\n",
        "train_images_for_model = [nucleus[0] for nucleus in all_train_labeled_nuclei]\n",
        "train_labels_for_model = [nucleus[1] for nucleus in all_train_labeled_nuclei]\n",
        "\n",
        "# Separate images and labels for testing\n",
        "test_images_for_model = [nucleus[0] for nucleus in all_test_labeled_nuclei]\n",
        "test_labels_for_model = [nucleus[1] for nucleus in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert images to NumPy arrays and add channel dimension\n",
        "train_images_for_model = np.array(train_images_for_model).reshape(-1, 64, 64, 1)\n",
        "test_images_for_model = np.array(test_images_for_model).reshape(-1, 64, 64, 1)\n",
        "\n",
        "# Convert labels to NumPy arrays and to numerical format\n",
        "label_mapping = {\"normal\": 0, \"damaged\": 1}\n",
        "train_labels_for_model = np.array([label_mapping[label] for label in train_labels_for_model])\n",
        "test_labels_for_model = np.array([label_mapping[label] for label in test_labels_for_model])\n",
        "\n",
        "# Print shapes\n",
        "print(f\"Shape of train_images_for_model: {train_images_for_model.shape}\")\n",
        "print(f\"Shape of train_labels_for_model: {train_labels_for_model.shape}\")\n",
        "print(f\"Shape of test_images_for_model: {test_images_for_model.shape}\")\n",
        "print(f\"Shape of test_labels_for_model: {test_labels_for_model.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed3c1b09"
      },
      "source": [
        "import mahotas\n",
        "from skimage.feature import hog\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import measure # Import measure for regionprops\n",
        "\n",
        "def extract_shape_features(image):\n",
        "    \"\"\"Extracts shape features from a binary nucleus mask using skimage.measure.regionprops.\"\"\"\n",
        "    # Ensure the image is binary (0 or 255) and has the correct dtype for regionprops\n",
        "    if image.max() <= 1:\n",
        "        image = image * 255\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Find contours (still needed for some calculations like perimeter and hull)\n",
        "    contours, _ = cv2.findContours(image.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    if not contours:\n",
        "        # Return a list of zeros if no contour is found\n",
        "        # Adjust size based on the features extracted by regionprops + Hu moments + HOG\n",
        "        # regionprops provides area, perimeter, major/minor axis length, eccentricity, solidity\n",
        "        # Hu moments are 7. HOG depends on parameters (e.g., 9 orientations for 1 cell block size 16x16)\n",
        "        return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Assuming the largest contour is the nucleus\n",
        "    contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Use regionprops for basic shape features\n",
        "    # Need to create a labeled image for regionprops\n",
        "    labeled_image, num_labels = measure.label(image, connectivity=2, return_num=True)\n",
        "\n",
        "    if num_labels < 1:\n",
        "         # Return a list of zeros if no labeled regions are found\n",
        "         return [0] * (5 + 7 + 9)\n",
        "\n",
        "    # Find properties of the largest region (assumed to be the nucleus)\n",
        "    properties = measure.regionprops(labeled_image)[0] # Assuming only one main region after segmentation\n",
        "\n",
        "    area = properties.area\n",
        "    perimeter = properties.perimeter\n",
        "    major_axis_length = properties.major_axis_length if properties.major_axis_length is not None else 0\n",
        "    minor_axis_length = properties.minor_axis_length if properties.minor_axis_length is not None else 0\n",
        "    axis_ratio = major_axis_length / minor_axis_length if minor_axis_length > 0 else 0\n",
        "    eccentricity = properties.eccentricity if properties.eccentricity is not None else 0\n",
        "    solidity = properties.solidity if properties.solidity is not None else 0\n",
        "\n",
        "    # Roundness/Form Factor (calculated using area and perimeter from regionprops)\n",
        "    roundness = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0\n",
        "\n",
        "    # Hu Moments (using cv2 on the contour)\n",
        "    hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()\n",
        "\n",
        "    # Histogram of Oriented Gradients (HOG) - using the image directly\n",
        "    try:\n",
        "        # HOG requires a specific input format and parameters. This is a basic example.\n",
        "        # The parameters for HOG (pixels_per_cell, cells_per_block, orientations) can be tuned.\n",
        "        hog_features = hog(image, pixels_per_cell=(16, 16), cells_per_block=(1, 1), orientations=9, feature_vector=True)\n",
        "    except ValueError:\n",
        "        hog_features = [0] * 9 # Example size, adjust based on HOG parameters\n",
        "\n",
        "    # Combine features\n",
        "    features = [area, perimeter, major_axis_length, minor_axis_length, axis_ratio, eccentricity, solidity, roundness] + list(hu_moments) + list(hog_features)\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for training and testing data\n",
        "train_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_shape_features = [extract_shape_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_shape_features = np.array(train_shape_features)\n",
        "test_shape_features = np.array(test_shape_features)\n",
        "\n",
        "print(f\"Shape of train_shape_features: {train_shape_features.shape}\")\n",
        "print(f\"Shape of test_shape_features: {test_shape_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da73477"
      },
      "source": [
        "import mahotas as mh\n",
        "from skimage import feature, measure\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian#, laplacian # Comment out laplacian import\n",
        "from skimage.feature import SIFT\n",
        "import scipy.ndimage # Import scipy for laplacian\n",
        "\n",
        "def extract_texture_features(image):\n",
        "    \"\"\"Extracts various texture features from a grayscale image.\"\"\"\n",
        "\n",
        "    # Ensure the image is in the correct format (uint8) for some libraries\n",
        "    image = image.astype(np.uint8)\n",
        "\n",
        "    # Check if the image is too small for feature extraction\n",
        "    if image.shape[0] < 21 or image.shape[1] < 21: # Some features like Gabor might need a minimum size\n",
        "         return [0] * (13 + 27 + 2 + 16 + 25 + 2 + 4*4) # Haralick + LBP + LoG (mean, std) + Gabor (4 orientations * 2 freqs * 2 features) + Zernike (degree 8) + SIFT (mean, std) + skimage haralick\n",
        "\n",
        "    # Haralick Texture Features (using mahotas) - keeping this as it was used before\n",
        "    # Check if the image is not all zeros or constant, which can cause issues with haralick\n",
        "    if np.all(image == image[0, 0]):\n",
        "        haralick_features = [0] * 13\n",
        "    else:\n",
        "        try:\n",
        "            haralick_features = mh.features.haralick(image).tolist()\n",
        "        except ValueError:\n",
        "            haralick_features = [0] * 13\n",
        "\n",
        "    # Local Binary Patterns (LBP) (using skimage) - from user's snippet\n",
        "    try:\n",
        "        lbp = feature.local_binary_pattern(image, P=8, R=1, method='uniform')\n",
        "        # Use a fixed number of bins for the histogram (26 uniform patterns + 2 for ends)\n",
        "        lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 28), density=True)\n",
        "        lbp_features = lbp_hist.tolist()\n",
        "    except Exception:\n",
        "        lbp_features = [0] * 27 # 26 uniform patterns + 1 for the remaining\n",
        "\n",
        "    # Haralick Texture Features (using scikit-image) - from user's snippet - using different distances/angles\n",
        "    try:\n",
        "        # Ensure image is uint8 for graycomatrix\n",
        "        image_uint8 = image.astype(np.uint8)\n",
        "        # Adjust distances and angles as needed\n",
        "        glcm = measure.graycomatrix(image_uint8, distances=[1, 3, 5], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4], symmetric=True, normed=True)\n",
        "        # Extract properties - using contrast, correlation, energy, homogeneity as in user's snippet\n",
        "        haralick_features_skimage = measure.graycoprops(glcm, props=['contrast', 'correlation', 'energy', 'homogeneity']).flatten().tolist()\n",
        "    except Exception:\n",
        "        haralick_features_skimage = [0] * (4 * 4) # 4 properties * 4 angles\n",
        "\n",
        "    # Laplacian of Gaussian (LoG) (using cv2) - from user's snippet\n",
        "    try:\n",
        "        # Apply Gaussian smoothing using cv2\n",
        "        blurred_image = cv2.GaussianBlur(image, (0, 0), 1)\n",
        "        # Apply Laplacian filter using cv2\n",
        "        log_image = cv2.Laplacian(blurred_image, cv2.CV_64F)\n",
        "        # Simple statistics from LoG response (e.g., mean and variance)\n",
        "        log_features = [np.mean(log_image), np.std(log_image)]\n",
        "    except Exception:\n",
        "        log_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Gabor Wavelets (using cv2) - from user's snippet\n",
        "    try:\n",
        "        gabor_features = []\n",
        "        # Define Gabor filter parameters (can be extended with different orientations and frequencies)\n",
        "        kernels = []\n",
        "        for theta in np.arange(0, np.pi, np.pi / 4): # 4 orientations\n",
        "            for freq in [5, 10]: # 2 frequencies\n",
        "                kern = cv2.getGaborKernel((21, 21), 5.0, theta, freq, 0.5, 0, ktype=cv2.CV_32F)\n",
        "                kernels.append(kern)\n",
        "\n",
        "        # Apply filters and extract features (e.g., mean and variance of the response)\n",
        "        for kernel in kernels:\n",
        "            fimg = cv2.filter2D(image, cv2.CV_8UC3, kernel) # Changed depth to CV_8UC3 as per documentation examples\n",
        "            # Convert to grayscale for mean/std calculation if fimg is BGR\n",
        "            if len(fimg.shape) == 3:\n",
        "                fimg = cv2.cvtColor(fimg, cv2.COLOR_BGR2GRAY)\n",
        "            gabor_features.extend([np.mean(fimg), np.std(fimg)])\n",
        "    except Exception:\n",
        "        gabor_features = [0] * (4 * 2 * 2) # 4 orientations * 2 frequencies * 2 features (mean, std)\n",
        "\n",
        "\n",
        "    # Zernike Moments (using mahotas) - keeping this\n",
        "    try:\n",
        "        # Ensure the image is binary for Zernike moments\n",
        "        # Use Otsu's thresholding on the current nucleus image ROI\n",
        "        _, binary_nucleus = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "        # Compute Zernike moments up to a certain degree (e.g., 8)\n",
        "        # The number of moments depends on the degree. For degree 8, there are 25 moments.\n",
        "        zernike_moments = mh.features.zernike_moments(binary_nucleus, radius=image.shape[0]//2, degree=8).tolist()\n",
        "    except Exception:\n",
        "         # Approximate number of Zernike moments for degree 8\n",
        "        zernike_moments = [0] * 25\n",
        "\n",
        "\n",
        "    # SIFT (Scale-Invariant Feature Transform) (using skimage) - keeping this\n",
        "    try:\n",
        "        # Initialize SIFT detector\n",
        "        # Consider adjusting parameters for feature detection\n",
        "        detector_extractor = SIFT()\n",
        "        # Find keypoints and compute descriptors\n",
        "        detector_extractor.detect_and_extract(image)\n",
        "        descriptors = detector_extractor.descriptors\n",
        "\n",
        "        # If no keypoints are found, descriptors will be None\n",
        "        if descriptors is not None:\n",
        "            # Simple representation: mean and standard deviation of descriptors\n",
        "            sift_features = [np.mean(descriptors), np.std(descriptors)]\n",
        "            # Or, you could cluster descriptors or use a bag-of-visual-words approach\n",
        "        else:\n",
        "            sift_features = [0] * 2 # Adjust size based on features extracted (e.g., mean, std)\n",
        "    except Exception:\n",
        "        sift_features = [0] * 2 # Adjust size based on features extracted\n",
        "\n",
        "\n",
        "    # Combine all texture features - including both mahotas and skimage haralick\n",
        "    features = haralick_features + lbp_features + haralick_features_skimage + log_features + gabor_features + zernike_moments + sift_features\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract texture features for training and testing data\n",
        "train_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_train_labeled_nuclei]\n",
        "test_texture_features = [extract_texture_features(nucleus_img) for nucleus_img, label in all_test_labeled_nuclei]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "# Ensure all feature lists have the same length before converting to numpy array\n",
        "def pad_features(features_list):\n",
        "    max_len = max(len(f) for f in features_list)\n",
        "    padded_features = []\n",
        "    for f in features_list:\n",
        "        # Ensure each element in f is a number before padding\n",
        "        padded_features.append([float(x) if isinstance(x, (int, float)) else 0.0 for x in f] + [0.0] * (max_len - len(f)))\n",
        "    return np.array(padded_features)\n",
        "\n",
        "train_texture_features = pad_features(train_texture_features)\n",
        "test_texture_features = pad_features(test_texture_features)\n",
        "\n",
        "\n",
        "print(f\"Shape of train_texture_features: {train_texture_features.shape}\")\n",
        "print(f\"Shape of test_texture_features: {test_texture_features.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "147d4415"
      },
      "source": [
        "# Combine shape and texture features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_shape_features.shape[0] == train_texture_features.shape[0]:\n",
        "    train_texture_features_flat = train_texture_features.reshape(train_texture_features.shape[0], -1)\n",
        "    train_combined_features = np.concatenate((train_shape_features, train_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of train_combined_features: {train_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for shape and texture features.\")\n",
        "    train_combined_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine shape and texture features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_shape_features.shape[0] == test_texture_features.shape[0]:\n",
        "    test_texture_features_flat = test_texture_features.reshape(test_texture_features.shape[0], -1)\n",
        "    test_combined_features = np.concatenate((test_shape_features, test_texture_features_flat), axis=1)\n",
        "    print(f\"Shape of test_combined_features: {test_combined_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_combined_features = None # Or handle the error appropriately"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd349317"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np # Ensure numpy is imported\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Import feature selection\n",
        "\n",
        "# Apply SMOTE to handle class imbalance on the combined training features\n",
        "if train_combined_features is not None and train_labels_for_model is not None:\n",
        "    smote = SMOTE(random_state=42)\n",
        "    train_features_smote, train_labels_smote = smote.fit_resample(train_combined_features, train_labels_for_model)\n",
        "\n",
        "    # Define the parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 200, 300],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'class_weight': [None, 'balanced', 'balanced_subsample'] # Experiment with class weighting\n",
        "    }\n",
        "\n",
        "    # Instantiate the Random Forest Classifier\n",
        "    rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1) # Optimize for recall\n",
        "\n",
        "    # Fit GridSearchCV to the SMOTE-resampled training data\n",
        "    grid_search.fit(train_features_smote, train_labels_smote)\n",
        "\n",
        "    # Get the best parameters and the best estimator\n",
        "    best_params = grid_search.best_params_\n",
        "    best_rf_classifier = grid_search.best_estimator_\n",
        "\n",
        "    print(\"Best parameters found by GridSearchCV:\")\n",
        "    print(best_params)\n",
        "\n",
        "    print(\"\\nOptimized Random Forest classifier trained successfully.\")\n",
        "\n",
        "    # Evaluate the best model on the original (non-SMOTE) test set\n",
        "    if test_combined_features is not None and test_labels_for_model is not None:\n",
        "        predictions = best_rf_classifier.predict(test_combined_features)\n",
        "        print(\"\\nOptimized Random Forest Classifier Evaluation on Test Set:\")\n",
        "        print(classification_report(test_labels_for_model, predictions))\n",
        "    else:\n",
        "        print(\"\\nTest data not available for evaluating the optimized Random Forest classifier.\")\n",
        "\n",
        "else:\n",
        "    print(\"Combined training features or labels not available for Random Forest optimization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f76bcf49"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = best_rf_classifier.predict(test_combined_features) # Use the best classifier\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"Optimized Random Forest Classifier Evaluation:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Compute and print accuracy score\n",
        "accuracy = accuracy_score(test_labels_for_model, predictions)\n",
        "print(f\"Accuracy Score: {accuracy}\")\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Extract feature importances from the best classifier\n",
        "feature_importances = best_rf_classifier.feature_importances_\n",
        "\n",
        "# Create a list of feature names (based on the order they were combined)\n",
        "# This requires knowing the number of features from each category\n",
        "num_shape_features = train_shape_features.shape[1]\n",
        "\n",
        "# Correctly calculate the number of flattened texture features\n",
        "# The number of texture features is simply the second dimension of the flattened array.\n",
        "# We can get this from the shape of train_texture_features_flat created in the previous step.\n",
        "try:\n",
        "    num_texture_features_flat = train_texture_features_flat.shape[1]\n",
        "except NameError:\n",
        "    # If train_texture_features_flat was not created, calculate its expected size\n",
        "    # based on the total combined features and shape features\n",
        "    if train_combined_features is not None and train_shape_features is not None:\n",
        "        num_texture_features_flat = train_combined_features.shape[1] - train_shape_features.shape[1]\n",
        "    else:\n",
        "        # Fallback if necessary variables are not defined\n",
        "        num_texture_features_flat = 0 # Default to 0 if cannot determine\n",
        "\n",
        "# Assuming a simple naming convention for demonstration\n",
        "shape_feature_names = [f\"shape_{i}\" for i in range(num_shape_features)]\n",
        "texture_feature_names = [f\"texture_{i}\" for i in range(num_texture_features_flat)]\n",
        "all_feature_names = shape_feature_names + texture_feature_names\n",
        "\n",
        "# Check if the lengths match before creating the Series\n",
        "if len(feature_importances) == len(all_feature_names):\n",
        "    # Create a pandas Series for feature importances\n",
        "    feature_importance_series = pd.Series(feature_importances, index=all_feature_names)\n",
        "\n",
        "    # Sort feature importances in descending order\n",
        "    sorted_feature_importances = feature_importance_series.sort_values(ascending=False)\n",
        "\n",
        "    # Print the top 20 most important features\n",
        "    print(\"\\nTop 20 Most Important Features:\")\n",
        "    print(sorted_feature_importances.head(20))\n",
        "else:\n",
        "    print(f\"Mismatch in feature counts: Model has {len(feature_importances)}, generated names have {len(all_feature_names)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d31acef"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Setup data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "# Fit the data generator on the training data\n",
        "datagen.fit(train_images_for_model) # Use the original train_images_for_model\n",
        "\n",
        "# Train the model using the augmented data\n",
        "history = model.fit(datagen.flow(train_images_for_model, train_labels_for_model, batch_size=32),\n",
        "                    epochs=50, # Number of training epochs\n",
        "                    validation_data=(test_images_for_model, test_labels_for_model)) # Use the original test_images_for_model and labels\n",
        "\n",
        "print(\"CNN model training completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eec430e1"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions_prob = model.predict(test_images_for_model)\n",
        "predictions = (predictions_prob > 0.5).astype(\"int32\")\n",
        "\n",
        "# Generate and print classification report\n",
        "print(\"CNN Classification Report:\")\n",
        "print(classification_report(test_labels_for_model, predictions))\n",
        "\n",
        "# Generate and print confusion matrix\n",
        "conf_matrix_cnn = confusion_matrix(test_labels_for_model, predictions)\n",
        "print(\"CNN Confusion Matrix:\")\n",
        "print(conf_matrix_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04635809"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Get predictions (probabilities) from the trained CNN on the training and testing data\n",
        "train_cnn_predictions_prob = model.predict(train_images_for_model) # Use the original train_images_for_model\n",
        "test_cnn_predictions_prob = model.predict(test_images_for_model) # Use the original test_images_for_model\n",
        "\n",
        "# Reshape CNN predictions to be 2D arrays\n",
        "train_cnn_predictions_prob = train_cnn_predictions_prob.reshape(-1, 1)\n",
        "test_cnn_predictions_prob = test_cnn_predictions_prob.reshape(-1, 1)\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for training\n",
        "# Ensure both arrays have the same number of samples\n",
        "if train_combined_features.shape[0] == train_cnn_predictions_prob.shape[0]:\n",
        "    train_ensemble_features = np.concatenate((train_combined_features, train_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of train_ensemble_features: {train_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of training samples for combined features and CNN predictions.\")\n",
        "    train_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# Combine CNN probabilities with handcrafted features for testing\n",
        "# Ensure both arrays have the same number of samples\n",
        "if test_combined_features.shape[0] == test_cnn_predictions_prob.shape[0]:\n",
        "    test_ensemble_features = np.concatenate((test_combined_features, test_cnn_predictions_prob), axis=1)\n",
        "    print(f\"Shape of test_ensemble_features: {test_ensemble_features.shape}\")\n",
        "else:\n",
        "    print(\"Mismatch in the number of testing samples for shape and texture features.\")\n",
        "    test_ensemble_features = None # Or handle the error appropriately\n",
        "\n",
        "\n",
        "# --- Feature Selection (Optional but Recommended for Ensemble) ---\n",
        "# Use SelectKBest to select the top K features based on ANOVA F-value\n",
        "# You can adjust the value of k (number of features to select)\n",
        "if train_ensemble_features is not None and train_labels_for_model is not None:\n",
        "    # Address potential NaNs or Infs in features before selection\n",
        "    train_ensemble_features_clean = np.nan_to_num(train_ensemble_features)\n",
        "\n",
        "    # Check if there are enough samples for feature selection\n",
        "    if train_ensemble_features_clean.shape[0] > 1 and np.var(train_ensemble_features_clean, axis=0).sum() > 0:\n",
        "        k = min(100, train_ensemble_features_clean.shape[1]) # Select top 100 features or fewer if less are available\n",
        "        try:\n",
        "            selector = SelectKBest(score_func=f_classif, k=k)\n",
        "            train_ensemble_features_selected = selector.fit_transform(train_ensemble_features_clean, train_labels_for_model)\n",
        "            test_ensemble_features_selected = selector.transform(np.nan_to_num(test_ensemble_features)) # Apply the same selection to test data\n",
        "            print(f\"Shape of train_ensemble_features_selected: {train_ensemble_features_selected.shape}\")\n",
        "            print(f\"Shape of test_ensemble_features_selected: {test_ensemble_features_selected.shape}\")\n",
        "        except ValueError as e:\n",
        "             print(f\"Could not perform feature selection: {e}\")\n",
        "             # Fallback to using all features if selection fails\n",
        "             train_ensemble_features_selected = train_ensemble_features_clean\n",
        "             test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "             print(\"Using all features for ensemble training.\")\n",
        "    else:\n",
        "         print(\"Not enough samples or variance for feature selection. Using all features.\")\n",
        "         train_ensemble_features_selected = train_ensemble_features_clean\n",
        "         test_ensemble_features_selected = np.nan_to_num(test_ensemble_features)\n",
        "else:\n",
        "    print(\"Ensemble features not available for selection.\")\n",
        "    train_ensemble_features_selected = None\n",
        "    test_ensemble_features_selected = None\n",
        "\n",
        "\n",
        "# --- Train Second-Level Random Forest Classifier ---\n",
        "# Apply SMOTE to the selected training features to handle class imbalance for the ensemble model\n",
        "if train_ensemble_features_selected is not None and train_labels_for_model is not None:\n",
        "    try:\n",
        "        smote_ensemble = SMOTE(random_state=42)\n",
        "        train_ensemble_features_smote, train_labels_ensemble_smote = smote_ensemble.fit_resample(train_ensemble_features_selected, train_labels_for_model)\n",
        "\n",
        "        # Instantiate and train the second-level Random Forest Classifier\n",
        "        ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        ensemble_classifier.fit(train_ensemble_features_smote, train_labels_ensemble_smote)\n",
        "\n",
        "        print(\"\\nEnsemble Random Forest classifier trained successfully with SMOTE.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Could not train ensemble classifier: {e}\")\n",
        "         ensemble_classifier = None\n",
        "else:\n",
        "    print(\"Could not train ensemble classifier due to missing data.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8ec6edb"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Make predictions on the test set using the ensemble model\n",
        "if test_ensemble_features_selected is not None and ensemble_classifier is not None:\n",
        "    ensemble_predictions = ensemble_classifier.predict(test_ensemble_features_selected)\n",
        "\n",
        "    # Generate and print classification report for the ensemble model\n",
        "    print(\"Ensemble Model Classification Report:\")\n",
        "    print(classification_report(test_labels_for_model, ensemble_predictions))\n",
        "\n",
        "    # Compute and print accuracy score for the ensemble model\n",
        "    ensemble_accuracy = accuracy_score(test_labels_for_model, ensemble_predictions)\n",
        "    print(f\"Ensemble Model Accuracy Score: {ensemble_accuracy}\")\n",
        "\n",
        "    # Generate and print confusion matrix for the ensemble model\n",
        "    conf_matrix_ensemble = confusion_matrix(test_labels_for_model, ensemble_predictions)\n",
        "    print(\"Ensemble Model Confusion Matrix:\")\n",
        "    print(conf_matrix_ensemble)\n",
        "else:\n",
        "    print(\"Ensemble model or test features not available for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}